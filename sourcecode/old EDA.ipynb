{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "df=pd.read_csv(r\"\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dublicates\n",
    "duplicated_data=df.duplicated()\n",
    "print(duplicated_data)\n",
    "\n",
    "duplicates_of_data = df[df.duplicated(keep=False)]\n",
    "print(duplicates_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null values\n",
    "df.isna()\n",
    "rows_with_nan = df[df.isna().any(axis=1)]\n",
    "print(rows_with_nan)\n",
    "\n",
    "print(df.isnull().sum())\n",
    "missing_percentage = df.isnull().mean() * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there is null values\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "# # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù‚ÙŠÙ… Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['Alcohol_Consumption'] = label_encoder.fit_transform(df['Alcohol_Consumption'])\n",
    "\n",
    "# # Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ ØªØ·Ø¨ÙŠÙ‚ IterativeImputer\n",
    "# imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "# df['Alcohol_Consumption'] = imputer.fit_transform(df[['Alcohol_Consumption']])\n",
    "\n",
    "# # Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
    "# print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†ØµÙŠØ©\n",
    "# df['Alcohol_Consumption_Original'] = df['Alcohol_Consumption']\n",
    "\n",
    "# # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†ØµÙŠØ© Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['Alcohol_Consumption'] = label_encoder.fit_transform(df['Alcohol_Consumption'])\n",
    "\n",
    "# # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¹Ù…ÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ\n",
    "# imputer = IterativeImputer(estimator=RandomForestClassifier(), max_iter=10, random_state=0)\n",
    "# df['Alcohol_Consumption'] = imputer.fit_transform(df[['Alcohol_Consumption']]).astype(int)\n",
    "\n",
    "# # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‚ÙŠÙ… Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¹Ù…ÙŠØ±\n",
    "# df['Alcohol_Consumption'] = label_encoder.inverse_transform(df['Alcohol_Consumption'])\n",
    "\n",
    "# # Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
    "# print(df[['Alcohol_Consumption_Original', 'Alcohol_Consumption']])\n",
    "\n",
    "\n",
    "\n",
    "# print(df.isnull().sum())  # Ù„Ù…Ø¹Ø±ÙØ© Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¹Ù…ÙŠØ±\n",
    "# print(df[df['Alcohol_Consumption'].isnull()])  # Ø¹Ø±Ø¶ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ Ù„Ù… ÙŠØªÙ… ØªØ¹Ù…ÙŠØ±Ù‡Ø§\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£ÙŠ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø© Ù…ØªØ¨Ù‚ÙŠØ©\n",
    "# simple_imputer = SimpleImputer(strategy='most_frequent')\n",
    "# df[['Alcohol_Consumption']] = simple_imputer.fit_transform(df[['Alcohol_Consumption']])\n",
    "# df.dropna(subset=['Alcohol_Consumption'], inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Alcohol_Consumption'].fillna(df['Alcohol_Consumption'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· ÙÙ‚Ø· Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ©\n",
    "# df.fillna(df.mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Blood Pressure to Systolic and Diastolic\n",
    "# df[['Systolic', 'Diastolic']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)\n",
    "# df.drop('Blood Pressure', axis=1, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ \"-\"\n",
    "# df[['age start', 'age end']] = df['Age_Category'].str.split('-', expand=True)\n",
    "\n",
    "# # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ \"+\"\n",
    "# df['age end'] = df['age end'].fillna(df['age start'])\n",
    "\n",
    "\n",
    "# # Ø­Ø°Ù Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ\n",
    "# df.drop('Age_Category', axis=1, inplace=True)\n",
    "\n",
    "# # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features don't affect on risk\n",
    "# Drop Unneeded columns\n",
    "# df.drop(['Sedentary Hours Per Day','Income','Country','Continent','Hemisphere','Sedentary Hours Per Day'],axis=1,inplace=True)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put target column data in last column\n",
    "new = df['HeartDisease']\n",
    "df.drop(columns=['HeartDisease'], axis=1, inplace=True)\n",
    "df['HeartDisease'] = new\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Age'\n",
    "\n",
    "mean_value = df[column_name].mean()\n",
    "\n",
    "df[column_name] = df[column_name].apply(lambda x: mean_value if x >= 100 else x)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#know datatype of data\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(\"Numeric Columns:\", numeric_columns)\n",
    "print(\"Categorical Columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers\n",
    "outliers_summary = {}\n",
    "for col in numeric_columns:\n",
    "    Q1 = df[col].quantile(0.25)  \n",
    "    Q3 = df[col].quantile(0.75)  \n",
    "    IQR = Q3 - Q1                  \n",
    "   \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outliers_count = outliers.shape[0]\n",
    "    outliers_summary[col] = outliers_count\n",
    "\n",
    "for col, count in outliers_summary.items():\n",
    "    print(f\"Column: {col}, Outliers: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ IQR\n",
    "Q1 = df['Fruit_Consumption'].quantile(0.25)\n",
    "Q3 = df['Fruit_Consumption'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø¹Ù„ÙŠØ§ ÙˆØ§Ù„Ø¯Ù†ÙŠØ§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ IQR\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© Ø¨Ø§Ù„ÙˆØ³ÙŠØ·\n",
    "median_value = df['Fruit_Consumption'].median()\n",
    "\n",
    "df['Fruit_Consumption'] = np.where(df['Fruit_Consumption'] < lower_bound, median_value, df['Fruit_Consumption'])\n",
    "df['Fruit_Consumption'] = np.where(df['Fruit_Consumption'] > upper_bound, median_value, df['Fruit_Consumption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of sex on Heart Attack Risk:\n",
    "pie_fig = px.pie(\n",
    "    df, names=\"Gender\", values=\"Alzheimer_risk\",\n",
    "    title=\"Effect of sex on Heart Attack Risk\"\n",
    ")\n",
    "pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Diabetes on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Diabetes\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Diabetes on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Family History on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Family History\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Family History on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Smoking on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Smoking\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Smoking on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Obesity on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Obesity\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Obesity on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Alcohol Consumption on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Alcohol Consumption\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Alcohol Consumption on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Previous Heart Problems on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Previous Heart Problems\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Previous Heart Problems on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Medication Use on Heart Attack Risk:\n",
    "# pie_fig = px.pie(\n",
    "#     df, names=\"Medication Use\", values=\"Heart Attack Risk\",\n",
    "#     title=\"Effect of Medication Use on Heart Attack Risk\"\n",
    "# )\n",
    "# pie_fig.update_traces(textposition='outside', textinfo='percent+label')\n",
    "# pie_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Effect of Continent on Heart Attack Risk:\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.barplot(x=df['Continent'], y=df['Heart Attack Risk'])\n",
    "# plt.title(\"Effect of Continent on Heart Attack Risk\")\n",
    "# plt.xlabel(\"Continent\")\n",
    "# plt.ylabel(\"Heart Attack Probability\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Encode categorical variables if necessary\n",
    "# le_Sex = LabelEncoder()\n",
    "# le_Diet = LabelEncoder()\n",
    "# le_Country= LabelEncoder()\n",
    "# le_Continent= LabelEncoder()\n",
    "# le_Hemisphere= LabelEncoder()\n",
    "# df['Sex'] = le_Sex.fit_transform(df['Sex']) \n",
    "# df['Diet'] =le_Diet.fit_transform(df['Diet'])\n",
    "# df['Country'] =le_Diet.fit_transform(df['Country'])\n",
    "# df['Continent'] =le_Diet.fit_transform(df['Continent'])\n",
    "# df['Hemisphere'] =le_Diet.fit_transform(df['Hemisphere'])\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ target column\n",
    "# numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ target column Ù…Ù† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ø¬ÙŠÙ…\n",
    "# target_column = 'Heart_Disease'  # Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù\n",
    "# numeric_columns.remove(target_column)\n",
    "\n",
    "# # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø¬ÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙÙ‚Ø·\n",
    "# scaler = StandardScaler()\n",
    "# df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØµÙÙˆÙ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙŠ ØªÙ… ØªØ­Ø¬ÙŠÙ…Ù‡Ø§\n",
    "# print(df[numeric_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
    "columns_to_convert = [\"Smoking\", \"Medication\", \"Obesity\", \"Heart_Attack\", \"Diabetes\", \"Family_History\", \"Angina\", \"Heart_Disease_History\"]\n",
    "\n",
    "# ØªØ­ÙˆÙŠÙ„ Ù‚ÙŠÙ… True/False Ø¥Ù„Ù‰ 1/0 ÙÙ‚Ø· ÙÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©\n",
    "df[columns_to_convert] = df[columns_to_convert].astype(int)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data scalling\n",
    "# numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "# scaler = StandardScaler()\n",
    "# df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "# print(df[numeric_columns].head())\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    df[col] = enc.fit_transform(df[col])\n",
    "\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯\n",
    "def apply_scaling_if_needed(df, column_name):\n",
    "    unique_values = df[column_name].unique()\n",
    "    \n",
    "    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ ÙŠØ­ØªÙˆÙŠ ÙÙ‚Ø· Ø¹Ù„Ù‰ 0 Ùˆ 1\n",
    "    if set(unique_values).issubset({0, 1}):\n",
    "        print(f\"Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„ØªØ·Ø¨ÙŠÙ‚ Scaling Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙˆØ¯ {column_name} Ù„Ø£Ù†Ù‡ ÙŠØ­ØªÙˆÙŠ ÙÙ‚Ø· Ø¹Ù„Ù‰ 0 Ùˆ 1\")\n",
    "    else:\n",
    "        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ø£Ø®Ø±Ù‰ (Ù…Ø«Ù„ 2 Ø£Ùˆ Ø£ÙƒØ«Ø±)\n",
    "        scaler = StandardScaler()\n",
    "        df[column_name] = scaler.fit_transform(df[[column_name]])\n",
    "        print(f\"ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Scaling Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙˆØ¯ {column_name}\")\n",
    "    \n",
    "# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
    "for column in df.columns:\n",
    "    apply_scaling_if_needed(df, column)\n",
    "\n",
    "# Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø¬ÙŠÙ…\n",
    "print(\"\\nØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø¬ÙŠÙ…:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove features don't affect on risk\n",
    "#Drop Unneeded columns\n",
    "# df.drop(['Occupation','Income_Level','Marital_Status','Education_Level','Urban_Rural','Region'],axis=1,inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "\n",
    "\n",
    "# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¥Ù„Ù‰ Ù†ÙˆØ¹ category\n",
    "categorical_features = ['Country', 'Gender', 'Education Level', 'Smoking Status', 'Alcohol Consumption',\n",
    "                        'Diabetes', 'Hypertension', 'Family History of Alzheimerâ€™s', 'Marital Status',\n",
    "                        'Genetic Risk Factor (APOE-Îµ4 allele)', 'Urban vs Rural Living', 'Employment Status']\n",
    "\n",
    "df[categorical_features] = df[categorical_features].astype('category')\n",
    "\n",
    "# Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ù„Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©\n",
    "numerical_features = ['Age', 'BMI', 'Physical Activity Level', 'Cholesterol Level', 'Cognitive Test Score',\n",
    "                      'Depression Level', 'Sleep Quality', 'Dietary Habits', 'Air Pollution Exposure',\n",
    "                      'Social Engagement Level', 'Income Level', 'Stress Levels']\n",
    "\n",
    "correlation_matrix = df[numerical_features + ['Alzheimer risk']].corr()\n",
    "\n",
    "# Ø±Ø³Ù… Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ù„Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Chi-Square Test\n",
    "chi2_results = {}\n",
    "for col in categorical_features:\n",
    "    contingency_table = pd.crosstab(df[col], df['Alzheimer risk'])\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    chi2_results[col] = p\n",
    "\n",
    "# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù‚ÙŠÙ… p-value (ÙƒÙ„Ù…Ø§ ÙƒØ§Ù†Øª Ø£ØµØºØ± Ù…Ù† 0.05ØŒ ÙƒØ§Ù†Øª Ø§Ù„Ù…ÙŠØ²Ø© Ù…Ø¤Ø«Ø±Ø©)\n",
    "print(\"Chi-Square Test p-values for Categorical Features:\")\n",
    "print(pd.Series(chi2_results).sort_values())\n",
    "\n",
    "# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ù„Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ANOVA\n",
    "anova_results = {}\n",
    "for col in numerical_features:\n",
    "    groups = [df[df['Alzheimer risk'] == val][col].dropna() for val in df['Alzheimer risk'].unique()]\n",
    "    f_stat, p = f_oneway(*groups)\n",
    "    anova_results[col] = p\n",
    "\n",
    "# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù‚ÙŠÙ… p-value\n",
    "print(\"ANOVA Test p-values for Numerical Features:\")\n",
    "print(pd.Series(anova_results).sort_values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('HeartDisease', axis=1) \n",
    "y = df['HeartDisease']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Random Forest Mean Squared Error:\", mean_squared_error(y_test, y_pred_rf))\n",
    "print(r2_score(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from xgboost import XGBClassifier\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "# m = df.drop('Heart_Attack', axis=1)\n",
    "# n = df['Heart_Attack']\n",
    "\n",
    "# # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±\n",
    "# Xtrain, Xtest, Ytrain, Ytest = train_test_split(m, n, test_size=0.30, random_state=1)\n",
    "\n",
    "# # ØªØ·Ø¨ÙŠÙ‚ SMOTE Ù„Ø²ÙŠØ§Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ù‚Ù„\n",
    "# sm = SMOTE(random_state=1)\n",
    "# Xtrain_res, Ytrain_res = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "# # 1ï¸âƒ£ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ RandomForest Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±\n",
    "# rf = RandomForestClassifier(class_weight='balanced', random_state=1, n_estimators=100, max_depth=10)\n",
    "# rf.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
    "# rf_predict = rf.predict(Xtest)\n",
    "\n",
    "# # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©\n",
    "# rf_train_acc = rf.score(Xtrain_res, Ytrain_res)\n",
    "# rf_test_acc = accuracy_score(Ytest, rf_predict)\n",
    "\n",
    "# # Ø·Ø¨Ø§Ø¹Ø© Ù†ØªØ§Ø¦Ø¬ RandomForest\n",
    "# print(\"Random Forest Training Accuracy:\", rf_train_acc)\n",
    "# print(\"Random Forest Test Accuracy:\", rf_test_acc)\n",
    "# print(\"Confusion Matrix for Random Forest:\\n\", confusion_matrix(Ytest, rf_predict))\n",
    "\n",
    "# # 2ï¸âƒ£ Ø§Ø³ØªØ®Ø¯Ø§Ù… XGBoost\n",
    "# xgb = XGBClassifier(scale_pos_weight=3, random_state=1)\n",
    "# xgb.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
    "# xgb_predict = xgb.predict(Xtest)\n",
    "\n",
    "# # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©\n",
    "# xgb_test_acc = accuracy_score(Ytest, xgb_predict)\n",
    "\n",
    "# # Ø·Ø¨Ø§Ø¹Ø© Ù†ØªØ§Ø¦Ø¬ XGBoost\n",
    "# print(\"XGBoost Test Accuracy:\", xgb_test_acc)\n",
    "# print(\"Confusion Matrix for XGBoost:\\n\", confusion_matrix(Ytest, xgb_predict))\n",
    "\n",
    "# # 3ï¸âƒ£ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¹ØªØ¨Ø© (Threshold) ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "# rf_prob = rf.predict_proba(Xtest)[:, 1]  # Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©\n",
    "# rf_predict_new = (rf_prob >= 0.3).astype(int)  # Ø¹ØªØ¨Ø© 30%\n",
    "\n",
    "# # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©\n",
    "# print(\"Confusion Matrix with adjusted threshold for Random Forest:\")\n",
    "# print(confusion_matrix(Ytest, rf_predict_new))\n",
    "\n",
    "# # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©\n",
    "# rf_test_acc_new = accuracy_score(Ytest, rf_predict_new)\n",
    "# print(\"Random Forest Test Accuracy with adjusted threshold:\", rf_test_acc_new)\n",
    "\n",
    "# # 4ï¸âƒ£ Cross-validation Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RandomForest Ùˆ XGBoost\n",
    "# rf_cv_scores = cross_val_score(rf, Xtrain_res, Ytrain_res, cv=5, scoring='accuracy')\n",
    "# xgb_cv_scores = cross_val_score(xgb, Xtrain_res, Ytrain_res, cv=5, scoring='accuracy')\n",
    "\n",
    "# print(\"Random Forest Cross-validation scores:\", rf_cv_scores)\n",
    "# print(\"XGBoost Cross-validation scores:\", xgb_cv_scores)\n",
    "\n",
    "# # 5ï¸âƒ£ Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Ø±Ø³Ù… Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(['Random Forest', 'XGBoost'], [rf_test_acc, xgb_test_acc])\n",
    "# plt.ylabel('Test Accuracy')\n",
    "# plt.title('Model Comparison')\n",
    "\n",
    "# # Ø±Ø³Ù… Ù†ØªØ§Ø¦Ø¬ cross-validation\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(['Random Forest', 'XGBoost'], [rf_cv_scores.mean(), xgb_cv_scores.mean()])\n",
    "# plt.ylabel('Cross-validation Accuracy')\n",
    "# plt.title('Cross-validation Comparison')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # ØªØ·Ø¨ÙŠÙ‚ SMOTE Ù„Ø²ÙŠØ§Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ù‚Ù„\n",
    "# sm = SMOTE(random_state=1)\n",
    "# Xtrain_res, Ytrain_res = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "# # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ XGBoost Ù…Ø¹ Ø¶Ø¨Ø· scale_pos_weight\n",
    "# xgb = XGBClassifier(scale_pos_weight=3, random_state=1)\n",
    "# xgb.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
    "# xgb_predict = xgb.predict(Xtest)\n",
    "\n",
    "# # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©\n",
    "# xgb_test_acc = accuracy_score(Ytest, xgb_predict)\n",
    "\n",
    "# # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "# print(\"XGBoost Test Accuracy:\", xgb_test_acc)\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(Ytest, xgb_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first model \"LogisticRegression\" without any improvement    veryyyyy gooooood\n",
    "\n",
    "logr=LogisticRegression()\n",
    "logr.fit(Xtrain,Ytrain)\n",
    "logR_predict=logr.predict(Xtest)\n",
    "Tr_score=logr.score(Xtrain,Ytrain)\n",
    "Tst_score=logr.score(Xtest,Ytest)\n",
    "\n",
    "print(\"Complete LogisticRegression Training !\",Tr_score)\n",
    "print(\"Complete LogisticRegression Test!\",Tst_score)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(Ytest, logR_predict))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(Ytest, logR_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #first model \"LogisticRegression\" with Smote and Class Weigh\n",
    "\n",
    "\n",
    "# # smote to increase number of smaller data\n",
    "# sm = SMOTE(random_state=1)\n",
    "# Xtrain_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# model= LogisticRegression(class_weight='balanced', random_state=1)\n",
    "# model.fit(Xtrain_res, y_train_res)\n",
    "\n",
    "# # Logistic Regression\n",
    "# model = LogisticRegression()\n",
    "# model.fit(Xtrain_res, y_train_res)\n",
    "\n",
    "# # logisticRegression\n",
    "# logR_predict = model.predict(X_test)\n",
    "\n",
    "# # accuracy\n",
    "# Tr_score = model.score(Xtrain_res, y_train_res)\n",
    "# Tst_score = model.score(X_test, y_test)\n",
    "\n",
    "# print(\"Complete LogisticRegression Training Accuracy:\", Tr_score)\n",
    "# print(\"Complete LogisticRegression Test Accuracy:\", Tst_score)\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, logR_predict))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, logR_predict))\n",
    "\n",
    "# plt.figure(figsize=(5, 4))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #first model \"LogisticRegression\" with Smote & Class Weigh & threshold & solver\n",
    "# Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "\n",
    "\n",
    "# # sm = SMOTE(random_state=1)\n",
    "# # Xtrain_res, Ytrain_res = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "\n",
    "# logR = LogisticRegression(class_weight=\"balanced\", solver='saga', random_state=1)\n",
    "# logR.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# y_probs = logR.predict_proba(Xtest)[:, 1]  \n",
    "\n",
    "\n",
    "# threshold = 0.5\n",
    "# logR_predict = (y_probs >= threshold).astype(int)\n",
    "\n",
    "# # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„ØªÙ‚Ø§Ø±ÙŠØ±\n",
    "# train_accuracy = accuracy_score(Ytrain_res, logR.predict(Xtrain_res))\n",
    "# test_accuracy = accuracy_score(Ytest, logR_predict)\n",
    "# roc_auc = roc_auc_score(Ytest, y_probs)\n",
    "\n",
    "# print(\"Complete LogisticRegression Training Accuracy:\", train_accuracy)\n",
    "# print(\"Complete LogisticRegression Test Accuracy:\", test_accuracy)\n",
    "# print(\"ROC-AUC Score:\", roc_auc)\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(Ytest, logR_predict))\n",
    "# print(\"\\nConfusion Matrix:\\n\", confusion_matrix(Ytest, logR_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # ØªØ·Ø¨ÙŠÙ‚ SMOTE\n",
    "# smote = SMOTE(sampling_strategy=0.8, random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Ø­Ø³Ø§Ø¨ scale_pos_weight ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§\n",
    "# scale_pos_weight = len(y_train_resampled[y_train_resampled == 0]) / len(y_train_resampled[y_train_resampled == 1])\n",
    "\n",
    "# # Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "# scaler = StandardScaler()\n",
    "# X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙˆÙ† Early Stopping\n",
    "# xgb = XGBClassifier(\n",
    "#     scale_pos_weight=scale_pos_weight,  \n",
    "#     random_state=42, \n",
    "#     max_depth=4,         \n",
    "#     learning_rate=0.05,  \n",
    "#     n_estimators=500,  \n",
    "#     subsample=0.8,      \n",
    "#     colsample_bytree=0.7,\n",
    "#     reg_alpha=0.1,      \n",
    "#     reg_lambda=1.0,\n",
    "#     eval_metric=\"logloss\"\n",
    "# )\n",
    "\n",
    "# # âœ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙˆÙ† early_stopping_rounds\n",
    "# xgb.fit(X_train_resampled, y_train_resampled, eval_set=[(X_test, y_test)], verbose=True)\n",
    "\n",
    "# # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª\n",
    "# y_probs = xgb.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# # Ø¶Ø¨Ø· Ø§Ù„Ø¹ØªØ¨Ø©\n",
    "# threshold = 0.6  \n",
    "# y_pred_xgb = (y_probs >= threshold).astype(int)\n",
    "\n",
    "# # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "# test_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "# roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# print(\"ğŸ”¹ XGBoost Accuracy:\", test_accuracy)\n",
    "# print(\"ğŸ”¹ ROC-AUC Score:\", roc_auc)\n",
    "# print(\"ğŸ”¹ Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "# print(\"ğŸ”¹ Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Ø¬Ø§Ù…Ø¯Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from imblearn.combine import SMOTEENN\n",
    "# from lightgbm import early_stopping\n",
    "\n",
    "# smote_enn = SMOTEENN(random_state=42)\n",
    "# X_train_res, y_train_res = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# # # Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SMOTE\n",
    "# # smote = SMOTE(random_state=42)\n",
    "# # X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # ØªØ¯Ø±ÙŠØ¨ LightGBM Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù„ØªØ¬Ù†Ø¨ Overfitting\n",
    "# lgbm = LGBMClassifier(\n",
    "#     num_leaves=6,              # ØªÙ‚Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
    "#     max_depth=1,                # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù‚\n",
    "#     learning_rate=0.03,         # Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… Ø£ØµØºØ± Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ù…ÙŠÙ…\n",
    "#     n_estimators=200,           # ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø¬Ø§Ø±\n",
    "#     reg_lambda=10.0,             # L2 Regularization\n",
    "#     reg_alpha=6,  \n",
    "#     min_child_samples=50,             # L1 Regularization\n",
    "#     bagging_fraction=0.6,       # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø¯Ø¯Ø©\n",
    "#     feature_fraction=0.6, \n",
    "#     bagging_freq=5,     # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù…ÙŠØ²Ø§Øª Ù…Ø¹ÙŠÙ†Ø©\n",
    "#     random_state=42\n",
    "# )\n",
    "# lgbm.fit(\n",
    "#     X_train_res, \n",
    "#     y_train_res,\n",
    "#     eval_set=[(X_test, y_test)],  \n",
    "#     eval_metric=\"logloss\",  \n",
    "#     callbacks=[early_stopping(stopping_rounds=20, verbose=True)]  \n",
    "    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# cv_scores = cross_val_score(lgbm, X_train_res, y_train_res, cv=5, scoring=\"accuracy\")\n",
    "# y_pred = lgbm.predict(X_test)\n",
    "# train_accuracy = accuracy_score(y_train_res, lgbm.predict(X_train_res))\n",
    "# test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(f\"Mean CV Accuracy: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Ø­Ø³Ø§Ø¨ Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¹Ù„Ù…\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    lgbm, X_train_res, y_train_res, cv=5, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· ÙˆØ§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„Ø¯Ù‚Ø©\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Ø±Ø³Ù… Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¹Ù„Ù…\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'b-', marker='o', label=\"Training Accuracy\")\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"blue\", alpha=0.2)\n",
    "\n",
    "plt.plot(train_sizes, val_mean, 'r-', marker='s', label=\"Validation Accuracy\")\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"red\", alpha=0.2)\n",
    "\n",
    "plt.title(\"Learning Curve (Detecting Overfitting)\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6= lgb.LGBMClassifier()\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model6, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10), scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_mean, label=\"Training Accuracy\", color=\"blue\")\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\"blue\")\n",
    "\n",
    "plt.plot(train_sizes, test_mean, label=\"Validation Accuracy\", color=\"red\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2, color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve (Detecting Overfitting)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ØªØ¹Ø¯ÙŠÙ„ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ LightGBM\n",
    "# param_grid = {\n",
    "    \n",
    "#     'num_leaves': [50, 100],  \n",
    "#     'max_depth': [10, 15],  \n",
    "#     'learning_rate': [0.05],  \n",
    "#     'n_estimators': [200, 300]  \n",
    "\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring='accuracy', cv=5, verbose=2)\n",
    "# grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù…Ø¹Ù„Ù…Ø§Øª\n",
    "# best_params = grid_search.best_params_\n",
    "# print(\"Ø£ÙØ¶Ù„ Ù…Ø¹Ù„Ù…Ø§Øª:\", best_params)\n",
    "\n",
    "# # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª\n",
    "# best_lgbm = grid_search.best_estimator_\n",
    "# y_pred_best_lgbm = best_lgbm.predict(X_test)\n",
    "\n",
    "# # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
    "# print(\"Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ LightGBM Classification Report:\\n\", classification_report(y_test, y_pred_best_lgbm))\n",
    "# print(\"Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ LightGBM Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best_lgbm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import lightgbm as lgb\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "# m = df.drop('diabetes', axis=1)\n",
    "# n = df['diabetes']\n",
    "\n",
    "# # ğŸ”¹ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±\n",
    "# Xtrain, Xtest, Ytrain, Ytest = train_test_split(m, n, test_size=0.30, random_state=1)\n",
    "\n",
    "# # ğŸ”¹ ØªØ·Ø¨ÙŠÙ‚ SMOTE Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„ÙØ¦Ø§Øª\n",
    "# sm = SMOTE(random_state=1)\n",
    "# Xtrain_res, Ytrain_res = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "# # ğŸ”¹ Ø¶Ø¨Ø· scale_pos_weight Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù†\n",
    "# scale_pos_weight = len(Ytrain[Ytrain == 0]) / len(Ytrain[Ytrain == 1])\n",
    "\n",
    "# # ğŸ”¹ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ LightGBM Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©\n",
    "# lgb_model = lgb.LGBMClassifier(\n",
    "#     scale_pos_weight=scale_pos_weight,  # Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„ÙØ¦Ø§Øª\n",
    "#     random_state=1,\n",
    "#     n_estimators=150, \n",
    "#     reg_alpha=20, \n",
    "#     reg_lambda=10, # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø¬Ø§Ø±\n",
    "#     max_depth=3,  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯\n",
    "#     learning_rate=0.05 ,\n",
    "#     feature_fraction=0.8, \n",
    "#     bagging_fraction=0.8, \n",
    "#     bagging_freq=5# Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…\n",
    "# )\n",
    "\n",
    "# # ğŸ”¹ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
    "# lgb_model.fit(Xtrain_res, Ytrain_res)\n",
    "\n",
    "# # ğŸ”¹ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„ÙØ¦Ø© 1\n",
    "# y_probs = lgb_model.predict_proba(Xtest)[:, 1]\n",
    "\n",
    "# # ğŸ”¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ø«Ù„Ù‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AUC-ROC\n",
    "# fpr, tpr, thresholds = roc_curve(Ytest, y_probs)\n",
    "# optimal_idx = np.argmax(tpr - fpr)  # Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ Ù†Ù‚Ø·Ø© ØªÙˆØ§Ø²Ù†\n",
    "# optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "# print(f\"ğŸ”¹ Optimal Threshold Found: {optimal_threshold:.3f}\")\n",
    "\n",
    "# # ğŸ”¹ Ø¶Ø¨Ø· Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ø«Ù„Ù‰\n",
    "# y_preds_adjusted = (y_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "# # ğŸ”¹ Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "# accuracy = accuracy_score(Ytest, y_preds_adjusted)\n",
    "# precision = precision_score(Ytest, y_preds_adjusted)\n",
    "# recall = recall_score(Ytest, y_preds_adjusted)\n",
    "# f1 = f1_score(Ytest, y_preds_adjusted)\n",
    "\n",
    "# # ğŸ”¹ Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "\n",
    "# train_accuracy = accuracy_score(Ytrain_res, lgb_model.predict(Xtrain_res))\n",
    "\n",
    "# # Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
    "# test_accuracy = accuracy_score(Ytest, lgb_model.predict(Xtest))\n",
    "\n",
    "# print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# print(\"\\nğŸ”¹ LightGBM Performance with Optimized Threshold ğŸ”¹\")\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1-Score: {f1:.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(Ytest, y_preds_adjusted))\n",
    "# print(\"Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ LightGBM Classification Report:\\n\", classification_report(Ytest,y_preds_adjusted))\n",
    "\n",
    "# # ğŸ”¹ Ø­Ø³Ø§Ø¨ Cross-validation\n",
    "# lgb_cv_scores = cross_val_score(lgb_model, Xtrain_res, Ytrain_res, cv=5, scoring='accuracy')\n",
    "# print(\"\\nğŸ”¹ LightGBM Cross-validation scores:\", lgb_cv_scores)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
